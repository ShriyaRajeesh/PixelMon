{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rzil-1/PixelMon/blob/main/Pixelmon1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9s7elDALqfeG",
        "outputId": "8458599d-807c-4089-c709-8a71dffd2b5f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Agent HP: 450 | Opponent HP: 410\n",
            "Agent HP: 400 | Opponent HP: 260\n",
            "Agent HP: 400 | Opponent HP: 260\n",
            "Agent HP: 250 | Opponent HP: 260\n",
            "Agent HP: 100 | Opponent HP: 260\n",
            "Agent HP: 0 | Opponent HP: 260\n",
            "Episode 1 finished with reward: -10\n",
            "Agent HP: 500 | Opponent HP: 390\n",
            "Agent HP: 500 | Opponent HP: 350\n",
            "Agent HP: 500 | Opponent HP: 310\n",
            "Agent HP: 445 | Opponent HP: 200\n",
            "Agent HP: 445 | Opponent HP: 90\n",
            "Agent HP: 390 | Opponent HP: 40\n",
            "Agent HP: 300 | Opponent HP: 0\n",
            "Episode 2 finished with reward: 10\n",
            "Agent HP: 460 | Opponent HP: 350\n",
            "Agent HP: 370 | Opponent HP: 200\n",
            "Agent HP: 280 | Opponent HP: 80\n",
            "Agent HP: 240 | Opponent HP: 40\n",
            "Agent HP: 200 | Opponent HP: 0\n",
            "Episode 3 finished with reward: 10\n",
            "Agent HP: 500 | Opponent HP: 460\n",
            "Agent HP: 350 | Opponent HP: 420\n",
            "Agent HP: 350 | Opponent HP: 270\n",
            "Agent HP: 200 | Opponent HP: 120\n",
            "Agent HP: 200 | Opponent HP: 10\n",
            "Agent HP: 200 | Opponent HP: 0\n",
            "Episode 4 finished with reward: 10\n",
            "Agent HP: 410 | Opponent HP: 380\n",
            "Agent HP: 365 | Opponent HP: 340\n",
            "Agent HP: 320 | Opponent HP: 220\n",
            "Agent HP: 320 | Opponent HP: 125\n",
            "Agent HP: 230 | Opponent HP: 30\n",
            "Agent HP: 230 | Opponent HP: 0\n",
            "Episode 5 finished with reward: 10\n",
            "Agent HP: 380 | Opponent HP: 410\n",
            "Agent HP: 260 | Opponent HP: 300\n",
            "Agent HP: 140 | Opponent HP: 210\n",
            "Agent HP: 140 | Opponent HP: 100\n",
            "Agent HP: 140 | Opponent HP: 100\n",
            "Agent HP: 140 | Opponent HP: 100\n",
            "Agent HP: 140 | Opponent HP: 100\n",
            "Agent HP: 140 | Opponent HP: 100\n",
            "Agent HP: 140 | Opponent HP: 100\n",
            "Agent HP: 140 | Opponent HP: 100\n",
            "Agent HP: 20 | Opponent HP: 100\n",
            "Agent HP: 20 | Opponent HP: 10\n",
            "Agent HP: 0 | Opponent HP: 0\n",
            "Episode 6 finished with reward: 10\n",
            "Agent HP: 350 | Opponent HP: 450\n",
            "Agent HP: 310 | Opponent HP: 390\n",
            "Agent HP: 250 | Opponent HP: 300\n",
            "Agent HP: 210 | Opponent HP: 250\n",
            "Agent HP: 170 | Opponent HP: 190\n",
            "Agent HP: 130 | Opponent HP: 140\n",
            "Agent HP: 70 | Opponent HP: 50\n",
            "Agent HP: 0 | Opponent HP: 0\n",
            "Episode 7 finished with reward: 10\n",
            "Agent HP: 460 | Opponent HP: 380\n",
            "Agent HP: 410 | Opponent HP: 260\n",
            "Agent HP: 260 | Opponent HP: 210\n",
            "Agent HP: 205 | Opponent HP: 90\n",
            "Agent HP: 55 | Opponent HP: 0\n",
            "Episode 8 finished with reward: 10\n",
            "Agent HP: 460 | Opponent HP: 440\n",
            "Agent HP: 395 | Opponent HP: 350\n",
            "Agent HP: 345 | Opponent HP: 260\n",
            "Agent HP: 305 | Opponent HP: 170\n",
            "Agent HP: 240 | Opponent HP: 110\n",
            "Agent HP: 200 | Opponent HP: 0\n",
            "Episode 9 finished with reward: 10\n",
            "Agent HP: 450 | Opponent HP: 350\n",
            "Agent HP: 385 | Opponent HP: 200\n",
            "Agent HP: 320 | Opponent HP: 120\n",
            "Agent HP: 255 | Opponent HP: 80\n",
            "Agent HP: 255 | Opponent HP: 40\n",
            "Agent HP: 135 | Opponent HP: 0\n",
            "Episode 10 finished with reward: 10\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import json\n",
        "from gym import Env\n",
        "from gym.spaces import Discrete, Box\n",
        "\n",
        "class PokemonBattleEnv(Env):\n",
        "    def __init__(self, pokemon_data_path=\"/content/pokemon_moves.json\"):\n",
        "        super(PokemonBattleEnv, self).__init__()\n",
        "\n",
        "        # Load Pokémon data from JSON\n",
        "        self.pokemon_data = self.load_pokemon_data(pokemon_data_path)\n",
        "\n",
        "        # Define action and observation space\n",
        "        self.action_space = Discrete(4)  # 4 moves per battle\n",
        "        self.observation_space = Box(low=0, high=500, shape=(2,), dtype=np.float32)\n",
        "\n",
        "        # Initialize variables\n",
        "        self.agent_hp = 100\n",
        "        self.opponent_hp = 100\n",
        "        self.agent_moves = []\n",
        "        self.opponent_moves = []\n",
        "        self.steps = 0\n",
        "\n",
        "    def load_pokemon_data(self, path):\n",
        "        with open(path, 'r') as f:\n",
        "            return json.load(f)\n",
        "\n",
        "    def reset(self):\n",
        "        self.agent_hp = 500\n",
        "        self.opponent_hp = 500\n",
        "        self.steps = 0\n",
        "\n",
        "        # Randomly select Pokémon and moves\n",
        "        agent_pokemon = random.choice(list(self.pokemon_data.keys()))\n",
        "        opponent_pokemon = random.choice(list(self.pokemon_data.keys()))\n",
        "\n",
        "        self.agent_moves = random.sample(list(self.pokemon_data[agent_pokemon].values()), 4)\n",
        "        self.opponent_moves = random.sample(list(self.pokemon_data[opponent_pokemon].values()), 4)\n",
        "\n",
        "        return np.array([self.agent_hp, self.opponent_hp], dtype=np.float32)\n",
        "\n",
        "    def step(self, action):\n",
        "        self.steps += 1\n",
        "\n",
        "        agent_damage = self.agent_moves[action]\n",
        "        self.opponent_hp -= agent_damage\n",
        "\n",
        "        opponent_action = random.randint(0, 3)\n",
        "        opponent_damage = self.opponent_moves[opponent_action]\n",
        "        self.agent_hp -= opponent_damage\n",
        "\n",
        "        self.agent_hp = max(self.agent_hp, 0)\n",
        "        self.opponent_hp = max(self.opponent_hp, 0)\n",
        "\n",
        "        reward = 0\n",
        "        damage_difference = agent_damage - opponent_damage\n",
        "\n",
        "        if self.opponent_hp == 0:\n",
        "            reward = 10  # Victory reward\n",
        "        elif self.agent_hp == 0:\n",
        "            reward = -10  # Loss penalty\n",
        "\n",
        "        done = self.agent_hp == 0 or self.opponent_hp == 0\n",
        "        return np.array([self.agent_hp, self.opponent_hp], dtype=np.float32), reward, done, {}\n",
        "\n",
        "    def render(self, mode='human'):\n",
        "        print(f\"Agent HP: {self.agent_hp} | Opponent HP: {self.opponent_hp}\")\n",
        "\n",
        "\n",
        "# Q-learning with State Space Reduction\n",
        "def discretize_hp(hp, bin_size=50):\n",
        "    \"\"\"Discretize HP into bins.\"\"\"\n",
        "    return int(hp // bin_size)\n",
        "\n",
        "num_actions = 4\n",
        "epsilon = 1.0  # Exploration rate\n",
        "epsilon_min = 0.01\n",
        "epsilon_decay = 0.995\n",
        "alpha = 0.9  # Learning rate\n",
        "gamma = 0.99  # Discount factor\n",
        "\n",
        "bin_size = 50  # HP discretization size\n",
        "num_bins = 500 // bin_size + 1  # Number of bins for HP\n",
        "env = PokemonBattleEnv(\"/content/pokemon_moves.json\")\n",
        "q_table = np.zeros((num_bins, num_bins, num_actions))  # Q-table for discretized states\n",
        "\n",
        "for episode in range(20000):\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "\n",
        "    while not done:\n",
        "        agent_hp, opponent_hp = state\n",
        "        state_idx = (discretize_hp(agent_hp, bin_size), discretize_hp(opponent_hp, bin_size))\n",
        "\n",
        "        if random.uniform(0, 1) < epsilon:\n",
        "            action = env.action_space.sample()  # Explore\n",
        "        else:\n",
        "            action = np.argmax(q_table[state_idx])  # Exploit\n",
        "\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "        next_state_idx = (discretize_hp(next_state[0], bin_size), discretize_hp(next_state[1], bin_size))\n",
        "\n",
        "        # Ensure the indices are within bounds before using them\n",
        "        if next_state_idx[0] >= num_bins or next_state_idx[1] >= num_bins:\n",
        "            next_state_idx = (min(next_state_idx[0], num_bins - 1), min(next_state_idx[1], num_bins - 1))\n",
        "\n",
        "        best_next_action = np.argmax(q_table[next_state_idx])\n",
        "        q_table[state_idx + (action,)] += alpha * (reward + gamma * q_table[next_state_idx + (best_next_action,)] - q_table[state_idx + (action,)])\n",
        "\n",
        "        state = next_state\n",
        "\n",
        "    epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
        "\n",
        "# Test the trained agent\n",
        "test_episodes = 10\n",
        "for episode in range(test_episodes):\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    while not done:\n",
        "        agent_hp, opponent_hp = state\n",
        "        state_idx = (discretize_hp(agent_hp, bin_size), discretize_hp(opponent_hp, bin_size))\n",
        "        action = np.argmax(q_table[state_idx])\n",
        "        state, reward, done, _ = env.step(action)\n",
        "        env.render()\n",
        "    print(f\"Episode {episode + 1} finished with reward: {reward}\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}